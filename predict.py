import cv2
import numpy as np
import pandas as pd
import dlib
import tensorflow as tf
from tensorflow.keras.preprocessing.image import img_to_array
from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D

# è‡ªæ‹å½±åƒæ•æ‰
def capture_selfie():
    """å•Ÿå‹•æ”å½±æ©Ÿï¼Œæ•æ‰ä¸€å¼µè‡ªæ‹ç…§"""
    camera = cv2.VideoCapture(0)
    if not camera.isOpened():
        print("âŒ ç„¡æ³•é–‹å•Ÿæ”å½±æ©Ÿ")
        return None

    print("ğŸ“¸ æ­£åœ¨æ•æ‰è‡ªæ‹ç…§...")
    ret, frame = camera.read()
    if ret:
        selfie_path = 'selfie.jpg'
        cv2.imwrite(selfie_path, frame)
        print("âœ… è‡ªæ‹ç…§ç‰‡å·²ä¿å­˜ç‚º selfie.jpg")
        cv2.imshow('Captured Selfie', frame)
        cv2.waitKey(2000)
        cv2.destroyAllWindows()
    else:
        print("âŒ ç„¡æ³•æ•æ‰å½±åƒ")
        selfie_path = None

    camera.release()
    return selfie_path

# æ€§åˆ¥åˆ¤å®š
gender_model = tf.keras.models.load_model('gender_classification_model.h5')

def predict_gender(image_path):
    """æ ¹æ“šè‡ªæ‹ç…§é æ¸¬æ€§åˆ¥"""
    image = cv2.imread(image_path)
    image_resized = cv2.resize(image, (224, 224))  # å‡è¨­æ¨¡å‹è¼¸å…¥å¤§å°ç‚º224x224
    image_normalized = np.expand_dims(image_resized, axis=0) / 255.0  # æ­£è¦åŒ–

    # é€²è¡Œæ€§åˆ¥é æ¸¬
    predictions = gender_model.predict(image_normalized)
    gender = "å¥³" if predictions[0][0] < 0.5 else "ç”·"
    
    print(f"é æ¸¬æ€§åˆ¥: {gender}")
    return gender

# è‡‰å‹åˆ†æ
def analyze_face_shape(image_path):
    """åˆ†æè‡‰å‹"""
    print("\nğŸ¤– æ­£åœ¨é€²è¡Œè‡‰å‹åˆ†æ...")
    image = cv2.imread(image_path)
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

    detector = dlib.get_frontal_face_detector()
    predictor = dlib.shape_predictor("shape_predictor_68_face_landmarks.dat")

    faces = detector(gray)
    if len(faces) == 0:
        print("âŒ æœªåµæ¸¬åˆ°äººè‡‰ï¼Œç„¡æ³•é€²è¡Œè‡‰å‹åˆ†æã€‚")
        return None

    for face in faces:
        landmarks = predictor(gray, face)
        jawline = [landmarks.part(i) for i in range(0, 17)]
        width = jawline[-1].x - jawline[0].x
        height = face.bottom() - face.top()

        aspect_ratio = height / width
        if aspect_ratio > 1.5:
            return "é•·å‹è‡‰"
        elif aspect_ratio > 1.2:
            return "æ©¢åœ“å½¢è‡‰"
        elif aspect_ratio > 1.0:
            return "åœ“å½¢è‡‰"
        else:
            return "æ–¹å½¢è‡‰"

# è¨“ç·´ MobileNet/ResNet æ¨¡å‹
def train_hairstyle_model():
    """è¨“ç·´é«®å‹åˆ†é¡æ¨¡å‹"""
    print("\nğŸ§  ä½¿ç”¨ MobileNetV2 è¨“ç·´é«®å‹åˆ†é¡æ¨¡å‹...")
    df = pd.read_csv('hairstyle_data.csv')
    image_paths = df['Image_Path'].values
    labels = df['Hairstyle_Name'].values

    features = []
    for img_path in image_paths:
        if os.path.exists(img_path):
            img = cv2.imread(img_path)
            img = cv2.resize(img, (224, 224))
            img = preprocess_input(img_to_array(img))
            features.append(img)
    
    X = np.array(features)
    y = LabelEncoder().fit_transform(labels)

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    predictions = Dense(len(np.unique(y)), activation='softmax')(x)

    model = Model(inputs=base_model.input, outputs=predictions)
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    
    model.fit(X_train, y_train, epochs=10, validation_split=0.2)
    accuracy = model.evaluate(X_test, y_test)[1]
    print(f"âœ… æ¨¡å‹æº–ç¢ºåº¦: {accuracy:.2f}")
    
    model.save('hairstyle_model.h5')  # å„²å­˜è¨“ç·´å¥½çš„æ¨¡å‹
    return model

# æ¨è–¦é«®å‹
def recommend_hairstyle(face_shape, gender, model):
    """æ¨è–¦é©åˆçš„é«®å‹"""
    print(f"\nğŸ¯ æ ¹æ“šè‡‰å‹ã€{face_shape}ã€åŠæ€§åˆ¥ã€{gender}ã€æ¨è–¦é©åˆçš„é«®å‹...")

    df = pd.read_csv('hairstyle_data.csv')
    filtered_df = df[df['Face_Shape'] == face_shape]
    
    # æ ¹æ“šæ€§åˆ¥ç¯©é¸é«®å‹
    filtered_df = filtered_df[filtered_df['Gender'] == gender]

    sample_img = cv2.imread(filtered_df['Image_Path'].iloc[0])
    sample_img = cv2.resize(sample_img, (224, 224))
    sample_img = preprocess_input(img_to_array(sample_img))
    sample_img = np.expand_dims(sample_img, axis=0)
    
    prediction = model.predict(sample_img)
    predicted_label = np.argmax(prediction)
    hairstyle_name = df['Hairstyle_Name'].iloc[predicted_label]
    
    print(f"âœ… æ¨è–¦é«®å‹ï¼š{hairstyle_name}")
    return filtered_df['Image_Path'].iloc[predicted_label]

# é«®å‹å¥—ç”¨èˆ‡å°æ¯”è¼¸å‡º
def apply_hairstyle(selfie_path, hairstyle_path):
    """å°‡æ¨è–¦é«®å‹å¥—ç”¨åˆ°è‡ªæ‹ç…§"""
    selfie = cv2.imread(selfie_path)
    hairstyle = cv2.imread(hairstyle_path)
    hairstyle = cv2.resize(hairstyle, (selfie.shape[1], selfie.shape[0]))

    # åœ¨å°æ¯”åœ–ä¸­åŠ ä¸Šæ¨™è¨»
    combined = np.hstack((selfie, hairstyle))
    
    font = cv2.FONT_HERSHEY_SIMPLEX
    cv2.putText(combined, 'Before', (10, 30), font, 1, (0, 0, 255), 2, cv2.LINE_AA)
    cv2.putText(combined, 'After', (selfie.shape[1] + 10, 30), font, 1, (0, 0, 255), 2, cv2.LINE_AA)

    result_path = 'result_comparison.jpg'
    cv2.imwrite(result_path, combined)
    print(f"âœ… å°æ¯”åœ–å·²ä¿å­˜ç‚º {result_path}")
    cv2.imshow('Before and After', combined)
    cv2.waitKey(0)
    cv2.destroyAllWindows()